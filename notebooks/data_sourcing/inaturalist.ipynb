{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "curr_dir = os.getcwd()\n",
    "\n",
    "def get_file_path(fname): \n",
    "    return os.path.join(curr_dir, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Carphophis amoenus', 'Cemophora coccinea', 'Coluber constrictor', 'Diadophis punctatus', 'Pantherophis guttatus', 'Pantherophis obsoletus', 'Farancia abacura', 'Farancia erytrogramma', 'Heterodon platirhinos', 'Heterodon simus', 'Lampropeltis rhombomaculata', 'Lampropeltis getula', 'Lampropeltis elapsoides', 'Lampropeltis triangulum triangulum', 'Masticophis flagellum', 'Nerodia erythrogaster', 'Nerodia fasciata', 'Nerodia sipedon', 'Nerodia taxispilota', 'Opheodrys aestivus', 'Pituophis melanoleucus', 'Regina rigida', 'Regina septemvitatta', 'Rhadinaea flavilata', 'Seminatrix pygaea', 'Storeria dekayi', 'Storeria occipitomaculata', 'Tantilla coronata', 'Thamnophis sauritus', 'Thamnophis sirtalis', 'Haldea striatula', 'Virginia valeriae', 'Micrurus fulvius', 'Agkistrodon contortrix', 'Agkistrodon piscivorus', 'Crotalus adamanteus', 'Crotalus horridus', 'Sistrurus miliarius']\n"
     ]
    }
   ],
   "source": [
    "snake_names_file = 'snakes.txt'\n",
    "try:\n",
    "    with open(get_file_path(snake_names_file), 'r') as file:\n",
    "        snake_names = file.readlines()\n",
    "        snake_names = [name.rstrip() for name in snake_names]\n",
    "        print(snake_names)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: snakes.txt not found, run nc_snakes notebook\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "snake_ids_file = 'snake_ids.json'\n",
    "\n",
    "api_base_url = \"https://api.inaturalist.org/v1\"\n",
    "get_observations_url = api_base_url + \"/observations\"\n",
    "\n",
    "def search_species_url(s_name):\n",
    "    return f\"{api_base_url}/search?q={s_name}&sources=taxa\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake ids file already exists, skipping...\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(get_file_path(snake_ids_file)):\n",
    "    print(\"Snake ids file already exists, skipping...\")\n",
    "    snake_ids = json.load(open(get_file_path(snake_ids_file), 'r'))\n",
    "else:\n",
    "    # Get map of snake names to species ids\n",
    "    snake_ids = {}\n",
    "\n",
    "    for snake_name in snake_names:\n",
    "        snake_ids[snake_name] = None\n",
    "\n",
    "        species_url = search_species_url(snake_name)\n",
    "        response = requests.get(species_url)\n",
    "        if response.status_code == 200:\n",
    "            species = response.json()['results']\n",
    "            if len(species) > 0:\n",
    "                snake_ids[snake_name] = species[0]['record']['id']\n",
    "            else:\n",
    "                print(f\"Error: {snake_name} not found in iNaturalist\")\n",
    "        else:\n",
    "            print(f\"Error {response.status_code} getting {snake_name} from iNaturalist\")\n",
    "\n",
    "            snake_ids\n",
    "\n",
    "    # save snake_to_species to file\n",
    "    with open('snake_to_species.json', 'w') as file:\n",
    "        json.dump(snake_ids, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import aiofiles\n",
    "\n",
    "async def download_file(session, folder, url, fname):\n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            if response.status == 200:\n",
    "                fpath = os.path.join(folder, fname)\n",
    "                async with aiofiles.open(fpath, \"wb\") as f:\n",
    "                    await f.write(await response.read())\n",
    "                    print(f\"Downloaded {url} to {fpath}\")\n",
    "            else:\n",
    "                print(f\"Error: {response.status} downloading {url}\")\n",
    "                print(response)\n",
    "    except aiohttp.ClientResponseError as e:\n",
    "        print(f\"Error: {e.status} downloading {url}\")\n",
    "\n",
    "async def download_all(folder, filenames_and_urls):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [download_file(session, folder, url, filename) for filename, url in filenames_and_urls]\n",
    "        await asyncio.gather(*tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Carphophis amoenus, folder already exists\n",
      "Skipping Cemophora coccinea, folder already exists\n",
      "Downloaded https://inaturalist-open-data.s3.amazonaws.com/photos/215104356/medium.jpg to /Users/jc/code/aipi540-module1/notebooks/data_sourcing/../../data/raw/coluber_constrictor-27137/215104356.jpg\n",
      "Downloaded https://inaturalist-open-data.s3.amazonaws.com/photos/215104747/medium.jpg to /Users/jc/code/aipi540-module1/notebooks/data_sourcing/../../data/raw/coluber_constrictor-27137/215104747.jpg\n"
     ]
    }
   ],
   "source": [
    "base_folder_path = os.path.join(curr_dir, \"../../data/raw\")\n",
    "if not os.path.exists(base_folder_path):\n",
    "    raise FileNotFoundError(\"Base folder path does not exist\")\n",
    "\n",
    "# To avoid downloading images again, set this to False\n",
    "load_images = True #False\n",
    "\n",
    "if load_images:\n",
    "    # Download images for each snake\n",
    "    for snake_name, snake_id in snake_ids.items():\n",
    "        if snake_id is None:\n",
    "            print(f\"Skipping {snake_name}, no id found\")\n",
    "            continue\n",
    "        \n",
    "        name_without_spaces = snake_name.replace(' ', '_').lower()\n",
    "        snake_dir = f\"{name_without_spaces}-{snake_id}\"\n",
    "        snake_folder_path = os.path.join(base_folder_path, snake_dir)\n",
    "\n",
    "        if not os.path.exists(snake_folder_path):\n",
    "            os.mkdir(snake_folder_path)\n",
    "        else:\n",
    "            print(f\"Skipping {snake_name}, folder already exists\")\n",
    "            continue\n",
    "\n",
    "        params = {\n",
    "            'verifiable': 'true',\n",
    "            'taxon_id': snake_id,\n",
    "            'order_by': 'votes',\n",
    "            'quality_grade': 'research',\n",
    "            'per_page': 1\n",
    "        }\n",
    "        \n",
    "        # Get observations for snake\n",
    "        response = requests.get(get_observations_url, params=params)\n",
    "\n",
    "        image_urls = {}\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            observations = response.json()['results']\n",
    "            for o in observations:\n",
    "                photos = o['photos']\n",
    "                for p in photos:\n",
    "                    image_url = p['url'].replace('square.', 'medium.')\n",
    "                    extension = image_url.split('/')[-1].split('.')[1]\n",
    "                    filename = f\"{p['id']}.{extension}\"\n",
    "                    image_urls[filename] = image_url\n",
    "        else:\n",
    "            print(f\"Error {response.status_code} getting observations for {snake_name}\")\n",
    "\n",
    "        await download_all(snake_folder_path, image_urls.items()) \n",
    "else:\n",
    "    print(\"Skipping image download, set load_images to True to download images\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
