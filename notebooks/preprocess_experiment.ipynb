{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirs = glob.glob('../data/snakes/*')\n",
    "data_dir = '../data/snakes/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the max image size\n",
    "def find_max_image_size(data_dir):\n",
    "    max_width, max_height = 0, 0\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.jpg'):\n",
    "                image_path = os.path.join(root, file)\n",
    "                with Image.open(image_path) as img:\n",
    "                    width, height = img.size\n",
    "                    max_width = max(max_width, width)\n",
    "                    max_height = max(max_height, height)\n",
    "    return max_width, max_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir, batch_size=32, resize=False, target_size=(224, 224)):\n",
    "    if resize:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(target_size),\n",
    "            transforms.Lambda(lambda x: transforms.functional.pad(x, (0, 0, target_size[0] - x.size[0], target_size[1] - x.size[1]), fill=0)),\n",
    "            transforms.CenterCrop(target_size),\n",
    "            transforms.ToTensor(),\n",
    "            # Use ImageNet mean and std temporarily, we can also calculate the mean and std of our dataset\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "        ])\n",
    "    else:\n",
    "        max_width, max_height = find_max_image_size(data_dir)\n",
    "        max_size = (max_width, max_height)\n",
    "        print('Max image size: {} x {}'.format(max_size[0], max_size[1]))\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(max_size),\n",
    "            transforms.Lambda(lambda x: transforms.functional.pad(x, (0, 0, max_size[0] - x.size[0], max_size[1] - x.size[1]), fill=0)),\n",
    "            transforms.ToTensor(),\n",
    "            # Use ImageNet mean and std temporarily, we can also calculate the mean and std of our dataset\n",
    "            # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "    # Splitting the dataset into training, validation, and testing\n",
    "    num_data = len(dataset)\n",
    "    print('Number of data: {}'.format(num_data))\n",
    "    indices = list(range(num_data))\n",
    "    split_train = int(0.6 * num_data)\n",
    "    split_val = int(0.8 * num_data)\n",
    "\n",
    "    # Shuffling indices\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Creating data samplers\n",
    "    train_idx, val_idx, test_idx = indices[:split_train], indices[split_train:split_val], indices[split_val:]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    val_sampler = SubsetRandomSampler(val_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "    # Creating data loaders\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "    loaders = {\n",
    "        'train': train_loader,\n",
    "        'valid': val_loader,\n",
    "        'test': test_loader\n",
    "    }\n",
    "\n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max image size: 500 x 500\n",
      "Number of data: 7753\n"
     ]
    }
   ],
   "source": [
    "## Usage example\n",
    "\n",
    "resize_option = False\n",
    "\n",
    "# Used if resize_option is True\n",
    "target_size = (224, 224)  \n",
    "\n",
    "loaders = load_data(data_dir, resize=resize_option, target_size=target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 4651\n",
      "Number of validation samples: 1551\n",
      "Number of test samples: 1551\n"
     ]
    }
   ],
   "source": [
    "print('Number of training samples: {}'.format(len(loaders['train'].sampler)))\n",
    "print('Number of validation samples: {}'.format(len(loaders['valid'].sampler)))\n",
    "print('Number of test samples: {}'.format(len(loaders['test'].sampler)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIPI520",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
